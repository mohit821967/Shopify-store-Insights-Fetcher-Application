{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohit821967/Shopify-store-Insights-Fetcher-Application/blob/main/Shopify_store_Insights_Fetcher_Application.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install fastapi uvicorn aiohttp beautifulsoup4 pydantic requests pandas numpy\n",
        "!pip install nest-asyncio python-multipart jinja2 python-dotenv lxml\n",
        "\n",
        "# Step 2: Import all necessary libraries\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Optional, Any, Union\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import nest_asyncio\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from pydantic import BaseModel, validator\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Enable nested event loops for Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "print(\"‚úÖ All dependencies installed and imported successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ToCYewIZ-f4",
        "outputId": "c0b58de8-3d1d-47a1-a12d-c277b0653d24"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.116.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.35.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (3.12.15)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.47.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.14.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.20.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.10.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (0.0.20)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (3.1.6)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2) (3.0.2)\n",
            "‚úÖ All dependencies installed and imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: UPDATED DATA MODELS WITH PROPER VALIDATION\n",
        "class Product(BaseModel):\n",
        "    id: int = 0\n",
        "    title: str = \"\"\n",
        "    handle: str = \"\"\n",
        "    description: Optional[str] = \"\"\n",
        "    vendor: str = \"\"\n",
        "    product_type: str = \"\"\n",
        "    price: float = 0.0\n",
        "    compare_at_price: Optional[float] = None\n",
        "    available: bool = True\n",
        "    tags: List[str] = []\n",
        "    images: List[str] = []\n",
        "\n",
        "    @validator('tags', pre=True)\n",
        "    def validate_tags(cls, v):\n",
        "        if isinstance(v, str):\n",
        "            return [tag.strip() for tag in v.split(',') if tag.strip()]\n",
        "        elif isinstance(v, list):\n",
        "            return [str(tag).strip() for tag in v if tag]\n",
        "        return []\n",
        "\n",
        "    @validator('price', pre=True)\n",
        "    def validate_price(cls, v):\n",
        "        try:\n",
        "            return float(v) if v is not None else 0.0\n",
        "        except (ValueError, TypeError):\n",
        "            return 0.0\n",
        "\n",
        "    @validator('compare_at_price', pre=True)\n",
        "    def validate_compare_at_price(cls, v):\n",
        "        if v is None or v == '' or v == 'null':\n",
        "            return None\n",
        "        try:\n",
        "            return float(v)\n",
        "        except (ValueError, TypeError):\n",
        "            return None\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "class SocialHandle(BaseModel):\n",
        "    platform: str\n",
        "    url: str\n",
        "    username: Optional[str] = None\n",
        "\n",
        "class ContactInfo(BaseModel):\n",
        "    emails: List[str] = []\n",
        "    phone_numbers: List[str] = []\n",
        "    address: Optional[str] = None\n",
        "\n",
        "class FAQ(BaseModel):\n",
        "    question: str\n",
        "    answer: str\n",
        "    category: Optional[str] = None\n",
        "\n",
        "class BrandInsights(BaseModel):\n",
        "    store_url: str\n",
        "    store_name: str\n",
        "    product_catalog: List[Product] = []\n",
        "    hero_products: List[Product] = []\n",
        "    privacy_policy: Optional[str] = None\n",
        "    return_policy: Optional[str] = None\n",
        "    refund_policy: Optional[str] = None\n",
        "    faqs: List[FAQ] = []\n",
        "    social_handles: List[SocialHandle] = []\n",
        "    contact_info: ContactInfo = ContactInfo()\n",
        "    about_brand: Optional[str] = None\n",
        "    important_links: Dict[str, str] = {}\n",
        "    scraped_at: datetime = datetime.now()\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "print(\"‚úÖ Data models defined successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r2a4Kz0Z-cj",
        "outputId": "f5abc923-edc5-496c-cd27-d3c5a30a0042"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Data models defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#STEP 4: UPDATED SCRAPING ENGINE WITH BUG FIXES\n",
        "class ShopifyStoreScraper:\n",
        "    def __init__(self):\n",
        "        self.session = None\n",
        "        self.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1'\n",
        "        }\n",
        "\n",
        "    async def __aenter__(self):\n",
        "        connector = aiohttp.TCPConnector(limit=30, limit_per_host=10)\n",
        "        self.session = aiohttp.ClientSession(\n",
        "            headers=self.headers,\n",
        "            timeout=aiohttp.ClientTimeout(total=30),\n",
        "            connector=connector\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
        "        if self.session:\n",
        "            await self.session.close()\n",
        "\n",
        "    async def scrape_store(self, store_url: str) -> BrandInsights:\n",
        "        \"\"\"Main scraping method that orchestrates all data collection\"\"\"\n",
        "        print(f\"üîç Starting to scrape: {store_url}\")\n",
        "\n",
        "        # Normalize URL\n",
        "        if not store_url.startswith('http'):\n",
        "            store_url = f'https://{store_url}'\n",
        "\n",
        "        try:\n",
        "            # Initialize insights object\n",
        "            insights = BrandInsights(\n",
        "                store_url=store_url,\n",
        "                store_name=self.extract_store_name(store_url)\n",
        "            )\n",
        "\n",
        "            # Scrape different components\n",
        "            print(\"üì¶ Scraping product catalog...\")\n",
        "            insights.product_catalog = await self.scrape_products(store_url)\n",
        "\n",
        "            print(\"üè† Scraping hero products...\")\n",
        "            insights.hero_products = await self.scrape_hero_products(store_url)\n",
        "\n",
        "            print(\"üìú Scraping policies...\")\n",
        "            policies = await self.scrape_policies(store_url)\n",
        "            insights.privacy_policy = policies.get('privacy')\n",
        "            insights.return_policy = policies.get('return')\n",
        "            insights.refund_policy = policies.get('refund')\n",
        "\n",
        "            print(\"‚ùì Scraping FAQs...\")\n",
        "            insights.faqs = await self.scrape_faqs(store_url)\n",
        "\n",
        "            print(\"üì± Scraping social handles...\")\n",
        "            insights.social_handles = await self.scrape_social_handles(store_url)\n",
        "\n",
        "            print(\"üìû Scraping contact info...\")\n",
        "            insights.contact_info = await self.scrape_contact_info(store_url)\n",
        "\n",
        "            print(\"‚ÑπÔ∏è Scraping about brand...\")\n",
        "            insights.about_brand = await self.scrape_about_brand(store_url)\n",
        "\n",
        "            print(\"üîó Scraping important links...\")\n",
        "            insights.important_links = await self.scrape_important_links(store_url)\n",
        "\n",
        "            print(\"‚úÖ Scraping completed successfully!\")\n",
        "            return insights\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error scraping store: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def extract_store_name(self, url: str) -> str:\n",
        "        \"\"\"Extract store name from URL\"\"\"\n",
        "        parsed = urlparse(url)\n",
        "        domain = parsed.netloc.replace('www.', '')\n",
        "        return domain.split('.')[0].title()\n",
        "\n",
        "    async def scrape_products(self, store_url: str) -> List[Product]:\n",
        "        \"\"\"Scrape product catalog using products.json endpoint with proper error handling\"\"\"\n",
        "        products = []\n",
        "        try:\n",
        "            # Try products.json endpoint first\n",
        "            products_url = f\"{store_url.rstrip('/')}/products.json\"\n",
        "            async with self.session.get(products_url) as response:\n",
        "                if response.status == 200:\n",
        "                    data = await response.json()\n",
        "\n",
        "                    for product_data in data.get('products', []):\n",
        "                        try:\n",
        "                            # Safely extract product information\n",
        "                            product_id = product_data.get('id', 0)\n",
        "                            title = product_data.get('title', '')\n",
        "                            handle = product_data.get('handle', '')\n",
        "                            description = self.clean_html(product_data.get('body_html', ''))\n",
        "                            vendor = product_data.get('vendor', '')\n",
        "                            product_type = product_data.get('product_type', '')\n",
        "                            available = product_data.get('available', True)\n",
        "\n",
        "                            # Handle tags properly - fix for the main error\n",
        "                            tags = product_data.get('tags', [])\n",
        "                            if isinstance(tags, str):\n",
        "                                tags = [tag.strip() for tag in tags.split(',') if tag.strip()]\n",
        "                            elif isinstance(tags, list):\n",
        "                                tags = [str(tag).strip() for tag in tags if tag]\n",
        "                            else:\n",
        "                                tags = []\n",
        "\n",
        "                            # Safely extract price information\n",
        "                            price = 0.0\n",
        "                            compare_at_price = None\n",
        "                            variants = product_data.get('variants', [])\n",
        "\n",
        "                            if variants and len(variants) > 0:\n",
        "                                first_variant = variants[0]\n",
        "                                try:\n",
        "                                    price = float(first_variant.get('price', 0))\n",
        "                                except (ValueError, TypeError):\n",
        "                                    price = 0.0\n",
        "\n",
        "                                compare_price_raw = first_variant.get('compare_at_price')\n",
        "                                if compare_price_raw and compare_price_raw != 'null':\n",
        "                                    try:\n",
        "                                        compare_at_price = float(compare_price_raw)\n",
        "                                    except (ValueError, TypeError):\n",
        "                                        compare_at_price = None\n",
        "\n",
        "                            # Extract images\n",
        "                            images = []\n",
        "                            for img in product_data.get('images', []):\n",
        "                                if isinstance(img, dict) and 'src' in img:\n",
        "                                    images.append(img['src'])\n",
        "                                elif isinstance(img, str):\n",
        "                                    images.append(img)\n",
        "\n",
        "                            # Create product object\n",
        "                            product = Product(\n",
        "                                id=product_id,\n",
        "                                title=title,\n",
        "                                handle=handle,\n",
        "                                description=description,\n",
        "                                vendor=vendor,\n",
        "                                product_type=product_type,\n",
        "                                price=price,\n",
        "                                compare_at_price=compare_at_price,\n",
        "                                available=available,\n",
        "                                tags=tags,\n",
        "                                images=images\n",
        "                            )\n",
        "                            products.append(product)\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"‚ö†Ô∏è Error parsing product {product_data.get('title', 'Unknown')}: {e}\")\n",
        "                            continue\n",
        "\n",
        "            print(f\"‚úÖ Found {len(products)} products\")\n",
        "            return products\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error scraping products: {e}\")\n",
        "            return []\n",
        "\n",
        "    async def scrape_hero_products(self, store_url: str) -> List[Product]:\n",
        "        \"\"\"Scrape hero products from homepage with better error handling\"\"\"\n",
        "        try:\n",
        "            async with self.session.get(store_url) as response:\n",
        "                if response.status == 200:\n",
        "                    html = await response.text()\n",
        "                    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "                    hero_products = []\n",
        "\n",
        "                    # Multiple selectors to find product links\n",
        "                    product_selectors = [\n",
        "                        'a[href*=\"/products/\"]',\n",
        "                        '.product-card a',\n",
        "                        '.product-item a',\n",
        "                        '[data-product-handle]',\n",
        "                        '.product-link',\n",
        "                        '.product a'\n",
        "                    ]\n",
        "\n",
        "                    seen_handles = set()\n",
        "\n",
        "                    for selector in product_selectors:\n",
        "                        try:\n",
        "                            links = soup.select(selector)[:8]  # Limit to first 8\n",
        "\n",
        "                            for link in links:\n",
        "                                href = link.get('href', '')\n",
        "                                if '/products/' in href:\n",
        "                                    # Extract product handle\n",
        "                                    handle = href.split('/products/')[-1].split('?')[0].split('#')[0]\n",
        "\n",
        "                                    if handle and handle not in seen_handles and len(handle) > 0:\n",
        "                                        seen_handles.add(handle)\n",
        "\n",
        "                                        # Try to get product data\n",
        "                                        try:\n",
        "                                            product_json_url = f\"{store_url.rstrip('/')}/products/{handle}.json\"\n",
        "                                            async with self.session.get(product_json_url) as prod_response:\n",
        "                                                if prod_response.status == 200:\n",
        "                                                    prod_data = await prod_response.json()\n",
        "                                                    product_info = prod_data.get('product', {})\n",
        "\n",
        "                                                    # Use the same parsing logic as main products\n",
        "                                                    tags = product_info.get('tags', [])\n",
        "                                                    if isinstance(tags, str):\n",
        "                                                        tags = [tag.strip() for tag in tags.split(',') if tag.strip()]\n",
        "                                                    elif isinstance(tags, list):\n",
        "                                                        tags = [str(tag).strip() for tag in tags if tag]\n",
        "                                                    else:\n",
        "                                                        tags = []\n",
        "\n",
        "                                                    # Get price safely\n",
        "                                                    price = 0.0\n",
        "                                                    variants = product_info.get('variants', [])\n",
        "                                                    if variants:\n",
        "                                                        try:\n",
        "                                                            price = float(variants[0].get('price', 0))\n",
        "                                                        except (ValueError, TypeError):\n",
        "                                                            price = 0.0\n",
        "\n",
        "                                                    hero_product = Product(\n",
        "                                                        id=product_info.get('id', 0),\n",
        "                                                        title=product_info.get('title', ''),\n",
        "                                                        handle=product_info.get('handle', handle),\n",
        "                                                        description=self.clean_html(product_info.get('body_html', '')),\n",
        "                                                        vendor=product_info.get('vendor', ''),\n",
        "                                                        product_type=product_info.get('product_type', ''),\n",
        "                                                        price=price,\n",
        "                                                        available=product_info.get('available', True),\n",
        "                                                        tags=tags,\n",
        "                                                        images=[img.get('src', '') for img in product_info.get('images', []) if isinstance(img, dict) and 'src' in img]\n",
        "                                                    )\n",
        "                                                    hero_products.append(hero_product)\n",
        "\n",
        "                                        except Exception as e:\n",
        "                                            print(f\"‚ö†Ô∏è Error fetching product {handle}: {e}\")\n",
        "                                            continue\n",
        "                        except Exception as e:\n",
        "                            print(f\"‚ö†Ô∏è Error with selector {selector}: {e}\")\n",
        "                            continue\n",
        "\n",
        "                    print(f\"‚úÖ Found {len(hero_products)} hero products\")\n",
        "                    return hero_products\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error scraping hero products: {e}\")\n",
        "            return []\n",
        "\n",
        "    async def scrape_policies(self, store_url: str) -> Dict[str, str]:\n",
        "        \"\"\"Scrape various policies with better error handling\"\"\"\n",
        "        policies = {}\n",
        "        policy_paths = {\n",
        "            'privacy': ['/pages/privacy-policy', '/policies/privacy-policy', '/pages/privacy', '/privacy-policy', '/privacy'],\n",
        "            'return': ['/pages/return-policy', '/policies/return-policy', '/pages/returns', '/return-policy', '/returns'],\n",
        "            'refund': ['/pages/refund-policy', '/policies/refund-policy', '/pages/refunds', '/refund-policy', '/refunds']\n",
        "        }\n",
        "\n",
        "        for policy_type, paths in policy_paths.items():\n",
        "            for path in paths:\n",
        "                try:\n",
        "                    policy_url = f\"{store_url.rstrip('/')}{path}\"\n",
        "                    async with self.session.get(policy_url) as response:\n",
        "                        if response.status == 200:\n",
        "                            html = await response.text()\n",
        "                            soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "                            # Remove unwanted elements\n",
        "                            for element in soup(['script', 'style', 'nav', 'header', 'footer']):\n",
        "                                element.decompose()\n",
        "\n",
        "                            # Find main content\n",
        "                            main_content = soup.find('main') or soup.find('div', class_=re.compile(r'content|main|policy'))\n",
        "\n",
        "                            if main_content:\n",
        "                                text = main_content.get_text(separator=' ', strip=True)\n",
        "                            else:\n",
        "                                text = soup.get_text(separator=' ', strip=True)\n",
        "\n",
        "                            cleaned_text = self.clean_text(text)\n",
        "\n",
        "                            if len(cleaned_text) > 100:  # Only store if substantial content\n",
        "                                policies[policy_type] = cleaned_text[:2000]  # Limit length\n",
        "                                break\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Error scraping {policy_type} policy from {path}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        return policies\n",
        "\n",
        "    async def scrape_faqs(self, store_url: str) -> List[FAQ]:\n",
        "        \"\"\"Scrape FAQs with improved parsing\"\"\"\n",
        "        faqs = []\n",
        "        faq_paths = ['/pages/faq', '/pages/faqs', '/faq', '/faqs', '/pages/frequently-asked-questions', '/help']\n",
        "\n",
        "        for path in faq_paths:\n",
        "            try:\n",
        "                faq_url = f\"{store_url.rstrip('/')}{path}\"\n",
        "                async with self.session.get(faq_url) as response:\n",
        "                    if response.status == 200:\n",
        "                        html = await response.text()\n",
        "                        soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "                        # Look for different FAQ structures\n",
        "                        faq_patterns = [\n",
        "                            ('details', 'summary'),  # Details/summary structure\n",
        "                            ('.faq-item', '.faq-question'),  # FAQ item structure\n",
        "                            ('.accordion-item', '.accordion-header'),  # Accordion structure\n",
        "                        ]\n",
        "\n",
        "                        for container_selector, question_selector in faq_patterns:\n",
        "                            containers = soup.select(container_selector)\n",
        "\n",
        "                            for container in containers:\n",
        "                                try:\n",
        "                                    question_elem = container.select_one(question_selector)\n",
        "                                    if question_elem:\n",
        "                                        question = question_elem.get_text(strip=True)\n",
        "\n",
        "                                        # Get answer - different methods\n",
        "                                        answer = \"\"\n",
        "                                        if container.name == 'details':\n",
        "                                            answer = container.get_text(strip=True).replace(question, '', 1)\n",
        "                                        else:\n",
        "                                            answer_elem = container.select_one('.faq-answer, .accordion-content, .answer')\n",
        "                                            if answer_elem:\n",
        "                                                answer = answer_elem.get_text(strip=True)\n",
        "\n",
        "                                        if question and answer and len(question) > 5 and len(answer) > 5:\n",
        "                                            faq = FAQ(\n",
        "                                                question=question[:300],\n",
        "                                                answer=answer[:800],\n",
        "                                                category=\"General\"\n",
        "                                            )\n",
        "                                            faqs.append(faq)\n",
        "\n",
        "                                except Exception as e:\n",
        "                                    print(f\"‚ö†Ô∏è Error parsing FAQ item: {e}\")\n",
        "                                    continue\n",
        "\n",
        "                        if faqs:\n",
        "                            break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error scraping FAQs from {path}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"‚úÖ Found {len(faqs)} FAQs\")\n",
        "        return faqs[:15]  # Limit to 15 FAQs\n",
        "\n",
        "    async def scrape_social_handles(self, store_url: str) -> List[SocialHandle]:\n",
        "        \"\"\"Scrape social media handles with better detection\"\"\"\n",
        "        social_handles = []\n",
        "\n",
        "        try:\n",
        "            async with self.session.get(store_url) as response:\n",
        "                if response.status == 200:\n",
        "                    html = await response.text()\n",
        "                    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "                    # Social media patterns\n",
        "                    social_patterns = {\n",
        "                        'instagram': [r'instagram\\.com/([^/?\\s]+)', r'instagr\\.am/([^/?\\s]+)'],\n",
        "                        'facebook': [r'facebook\\.com/([^/?\\s]+)', r'fb\\.com/([^/?\\s]+)'],\n",
        "                        'twitter': [r'twitter\\.com/([^/?\\s]+)', r'x\\.com/([^/?\\s]+)'],\n",
        "                        'youtube': [r'youtube\\.com/([^/?\\s]+)', r'youtu\\.be/([^/?\\s]+)'],\n",
        "                        'tiktok': [r'tiktok\\.com/([^/?\\s]+)'],\n",
        "                        'linkedin': [r'linkedin\\.com/([^/?\\s]+)'],\n",
        "                        'pinterest': [r'pinterest\\.com/([^/?\\s]+)'],\n",
        "                        'snapchat': [r'snapchat\\.com/([^/?\\s]+)']\n",
        "                    }\n",
        "\n",
        "                    # Find all links\n",
        "                    links = soup.find_all('a', href=True)\n",
        "\n",
        "                    seen_platforms = set()\n",
        "\n",
        "                    for link in links:\n",
        "                        href = link.get('href', '')\n",
        "\n",
        "                        for platform, patterns in social_patterns.items():\n",
        "                            if platform not in seen_platforms:\n",
        "                                for pattern in patterns:\n",
        "                                    match = re.search(pattern, href, re.IGNORECASE)\n",
        "                                    if match:\n",
        "                                        username = match.group(1)\n",
        "                                        # Clean username\n",
        "                                        username = username.split('?')[0].split('#')[0].strip('/')\n",
        "\n",
        "                                        if username and len(username) > 0:\n",
        "                                            social_handle = SocialHandle(\n",
        "                                                platform=platform,\n",
        "                                                url=href,\n",
        "                                                username=username\n",
        "                                            )\n",
        "                                            social_handles.append(social_handle)\n",
        "                                            seen_platforms.add(platform)\n",
        "                                            break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error scraping social handles: {e}\")\n",
        "\n",
        "        print(f\"‚úÖ Found {len(social_handles)} social handles\")\n",
        "        return social_handles\n",
        "\n",
        "    async def scrape_contact_info(self, store_url: str) -> ContactInfo:\n",
        "        \"\"\"Scrape contact information with better extraction\"\"\"\n",
        "        contact_info = ContactInfo()\n",
        "\n",
        "        try:\n",
        "            # Try contact page first, then homepage\n",
        "            contact_urls = [\n",
        "                f\"{store_url.rstrip('/')}/pages/contact\",\n",
        "                f\"{store_url.rstrip('/')}/contact\",\n",
        "                store_url\n",
        "            ]\n",
        "\n",
        "            html_content = \"\"\n",
        "            for url in contact_urls:\n",
        "                try:\n",
        "                    async with self.session.get(url) as response:\n",
        "                        if response.status == 200:\n",
        "                            html_content = await response.text()\n",
        "                            break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if html_content:\n",
        "                # Improved email regex\n",
        "                email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "                emails = re.findall(email_pattern, html_content)\n",
        "\n",
        "                # Improved phone regex\n",
        "                phone_patterns = [\n",
        "                    r'(\\+?91[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',  # Indian format\n",
        "                    r'(\\+?1[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',   # US format\n",
        "                    r'(\\+?44[-.\\s]?)?\\(?\\d{3,4}\\)?[-.\\s]?\\d{3,4}[-.\\s]?\\d{4}' # UK format\n",
        "                ]\n",
        "\n",
        "                phones = []\n",
        "                for pattern in phone_patterns:\n",
        "                    matches = re.findall(pattern, html_content)\n",
        "                    phones.extend(matches)\n",
        "\n",
        "                # Clean and deduplicate\n",
        "                unique_emails = list(set([email.lower() for email in emails if '@' in email and '.' in email]))\n",
        "                unique_phones = list(set([phone if isinstance(phone, str) else '-'.join(phone) for phone in phones]))\n",
        "\n",
        "                contact_info.emails = unique_emails[:3]  # Limit to 3 emails\n",
        "                contact_info.phone_numbers = unique_phones[:3]  # Limit to 3 phones\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error scraping contact info: {e}\")\n",
        "\n",
        "        return contact_info\n",
        "\n",
        "    async def scrape_about_brand(self, store_url: str) -> Optional[str]:\n",
        "        \"\"\"Scrape about brand information with better content extraction\"\"\"\n",
        "        about_paths = ['/pages/about', '/pages/about-us', '/pages/our-story', '/about', '/about-us', '/our-story']\n",
        "\n",
        "        for path in about_paths:\n",
        "            try:\n",
        "                about_url = f\"{store_url.rstrip('/')}{path}\"\n",
        "                async with self.session.get(about_url) as response:\n",
        "                    if response.status == 200:\n",
        "                        html = await response.text()\n",
        "                        soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "                        # Remove unwanted elements\n",
        "                        for element in soup(['script', 'style', 'nav', 'header', 'footer']):\n",
        "                            element.decompose()\n",
        "\n",
        "                        # Find main content areas\n",
        "                        content_selectors = [\n",
        "                            'main',\n",
        "                            '.about-content',\n",
        "                            '.content',\n",
        "                            '.main-content',\n",
        "                            '#main-content',\n",
        "                            '.page-content'\n",
        "                        ]\n",
        "\n",
        "                        main_content = None\n",
        "                        for selector in content_selectors:\n",
        "                            main_content = soup.select_one(selector)\n",
        "                            if main_content:\n",
        "                                break\n",
        "\n",
        "                        if not main_content:\n",
        "                            main_content = soup.find('body')\n",
        "\n",
        "                        if main_content:\n",
        "                            text = main_content.get_text(separator=' ', strip=True)\n",
        "                            cleaned_text = self.clean_text(text)\n",
        "\n",
        "                            if len(cleaned_text) > 100:\n",
        "                                return cleaned_text[:1500]  # Limit to 1500 characters\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error scraping about from {path}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return None\n",
        "\n",
        "    async def scrape_important_links(self, store_url: str) -> Dict[str, str]:\n",
        "        \"\"\"Scrape important links with better detection\"\"\"\n",
        "        important_links = {}\n",
        "\n",
        "        try:\n",
        "            async with self.session.get(store_url) as response:\n",
        "                if response.status == 200:\n",
        "                    html = await response.text()\n",
        "                    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "                    # Important link keywords\n",
        "                    important_keywords = {\n",
        "                        'track': ['track', 'tracking', 'order tracking'],\n",
        "                        'contact': ['contact', 'contact us', 'get in touch'],\n",
        "                        'blog': ['blog', 'news', 'articles'],\n",
        "                        'size': ['size', 'size guide', 'sizing'],\n",
        "                        'shipping': ['shipping', 'delivery', 'shipping policy'],\n",
        "                        'support': ['support', 'help', 'customer service'],\n",
        "                        'returns': ['returns', 'return policy'],\n",
        "                        'careers': ['careers', 'jobs', 'work with us']\n",
        "                    }\n",
        "\n",
        "                    links = soup.find_all('a', href=True)\n",
        "\n",
        "                    for link in links:\n",
        "                        href = link.get('href', '')\n",
        "                        text = link.get_text(strip=True).lower()\n",
        "\n",
        "                        if len(text) > 50:  # Skip very long texts\n",
        "                            continue\n",
        "\n",
        "                        for category, keywords in important_keywords.items():\n",
        "                            if any(keyword in text for keyword in keywords):\n",
        "                                if href.startswith('/'):\n",
        "                                    href = f\"{store_url.rstrip('/')}{href}\"\n",
        "                                elif not href.startswith('http'):\n",
        "                                    href = f\"{store_url.rstrip('/')}/{href}\"\n",
        "\n",
        "                                clean_text = text.title()\n",
        "                                if clean_text not in important_links:\n",
        "                                    important_links[clean_text] = href\n",
        "                                break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error scraping important links: {e}\")\n",
        "\n",
        "        return important_links\n",
        "\n",
        "    def clean_html(self, html_string: str) -> str:\n",
        "        \"\"\"Clean HTML content\"\"\"\n",
        "        if not html_string:\n",
        "            return \"\"\n",
        "        soup = BeautifulSoup(html_string, 'html.parser')\n",
        "        return soup.get_text(separator=' ', strip=True)\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize text\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "        # Remove extra whitespace and normalize\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'\\n+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "print(\"‚úÖ Updated scraping engine defined successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-ROy8HLZ-aZ",
        "outputId": "37ebc110-5758-413f-ce34-9b880023e4a8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Updated scraping engine defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: MAIN APPLICATION INTERFACE\n",
        "\n",
        "class ShopifyInsightsFetcher:\n",
        "    def __init__(self):\n",
        "        self.scraper = ShopifyStoreScraper()\n",
        "\n",
        "    async def analyze_store(self, store_url: str) -> BrandInsights:\n",
        "        \"\"\"Analyze a single Shopify store\"\"\"\n",
        "        async with self.scraper as scraper:\n",
        "            return await scraper.scrape_store(store_url)\n",
        "\n",
        "    def display_insights(self, insights: BrandInsights):\n",
        "        \"\"\"Display insights in a formatted way\"\"\"\n",
        "        print(\"=\"*80)\n",
        "        print(f\"üè™ STORE ANALYSIS: {insights.store_name}\")\n",
        "        print(f\"üåê URL: {insights.store_url}\")\n",
        "        print(f\"üìÖ Scraped: {insights.scraped_at}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(f\"\\nüì¶ PRODUCT CATALOG ({len(insights.product_catalog)} products)\")\n",
        "        if insights.product_catalog:\n",
        "            for i, product in enumerate(insights.product_catalog[:5]):  # Show first 5\n",
        "                print(f\"  {i+1}. {product.title}\")\n",
        "                print(f\"     Price: ${product.price} | Vendor: {product.vendor}\")\n",
        "                print(f\"     Type: {product.product_type} | Available: {product.available}\")\n",
        "                if product.tags:\n",
        "                    print(f\"     Tags: {', '.join(product.tags[:3])}\")\n",
        "                print()\n",
        "            if len(insights.product_catalog) > 5:\n",
        "                print(f\"  ... and {len(insights.product_catalog) - 5} more products\")\n",
        "\n",
        "        print(f\"\\nüè† HERO PRODUCTS ({len(insights.hero_products)} products)\")\n",
        "        if insights.hero_products:\n",
        "            for i, product in enumerate(insights.hero_products):\n",
        "                print(f\"  {i+1}. {product.title} - ${product.price}\")\n",
        "\n",
        "        print(f\"\\nüì± SOCIAL HANDLES ({len(insights.social_handles)} handles)\")\n",
        "        for handle in insights.social_handles:\n",
        "            print(f\"  {handle.platform.title()}: {handle.url}\")\n",
        "            if handle.username:\n",
        "                print(f\"    Username: @{handle.username}\")\n",
        "\n",
        "        print(f\"\\nüìû CONTACT INFO\")\n",
        "        if insights.contact_info.emails:\n",
        "            print(f\"  üìß Emails: {', '.join(insights.contact_info.emails)}\")\n",
        "        if insights.contact_info.phone_numbers:\n",
        "            print(f\"  üì± Phones: {', '.join(insights.contact_info.phone_numbers)}\")\n",
        "\n",
        "        print(f\"\\n‚ùì FAQs ({len(insights.faqs)} found)\")\n",
        "        for i, faq in enumerate(insights.faqs[:3]):  # Show first 3\n",
        "            print(f\"  Q{i+1}: {faq.question}\")\n",
        "            print(f\"  A{i+1}: {faq.answer[:150]}...\")\n",
        "            print()\n",
        "\n",
        "        print(f\"\\nüîó IMPORTANT LINKS ({len(insights.important_links)} found)\")\n",
        "        for name, url in list(insights.important_links.items())[:5]:\n",
        "            print(f\"  {name}: {url}\")\n",
        "\n",
        "        print(f\"\\nüìú POLICIES\")\n",
        "        if insights.privacy_policy:\n",
        "            print(f\"  ‚úÖ Privacy Policy: {len(insights.privacy_policy)} characters\")\n",
        "        if insights.return_policy:\n",
        "            print(f\"  ‚úÖ Return Policy: {len(insights.return_policy)} characters\")\n",
        "        if insights.refund_policy:\n",
        "            print(f\"  ‚úÖ Refund Policy: {len(insights.refund_policy)} characters\")\n",
        "\n",
        "        if insights.about_brand:\n",
        "            print(f\"\\n‚ÑπÔ∏è ABOUT BRAND\")\n",
        "            print(f\"  {insights.about_brand[:300]}...\")\n",
        "\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    def export_to_json(self, insights: BrandInsights, filename: str = None):\n",
        "        \"\"\"Export insights to JSON file\"\"\"\n",
        "        if not filename:\n",
        "            filename = f\"{insights.store_name}_insights.json\"\n",
        "\n",
        "        # Convert to dict for JSON serialization\n",
        "        insights_dict = insights.dict()\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(insights_dict, f, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "        print(f\"üìÅ Insights exported to {filename}\")\n",
        "        return filename\n",
        "\n",
        "    def create_summary_dataframe(self, insights_list: List[BrandInsights]) -> pd.DataFrame:\n",
        "        \"\"\"Create a summary DataFrame from multiple insights\"\"\"\n",
        "        summary_data = []\n",
        "\n",
        "        for insights in insights_list:\n",
        "            summary_data.append({\n",
        "                'Store Name': insights.store_name,\n",
        "                'URL': insights.store_url,\n",
        "                'Total Products': len(insights.product_catalog),\n",
        "                'Hero Products': len(insights.hero_products),\n",
        "                'Social Handles': len(insights.social_handles),\n",
        "                'FAQs': len(insights.faqs),\n",
        "                'Has Privacy Policy': bool(insights.privacy_policy),\n",
        "                'Has Return Policy': bool(insights.return_policy),\n",
        "                'Has About Section': bool(insights.about_brand),\n",
        "                'Contact Emails': len(insights.contact_info.emails),\n",
        "                'Contact Phones': len(insights.contact_info.phone_numbers),\n",
        "                'Important Links': len(insights.important_links),\n",
        "                'Scraped At': insights.scraped_at.strftime('%Y-%m-%d %H:%M:%S')\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(summary_data)\n",
        "\n",
        "print(\"‚úÖ Main application interface ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HC_M1c6sZ-YC",
        "outputId": "09cb7b10-b44a-4934-9f97-5acc3d211662"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Main application interface ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: DEMO FUNCTIONS\n",
        "\n",
        "# Initialize the fetcher\n",
        "fetcher = ShopifyInsightsFetcher()\n",
        "\n",
        "# Demo 1: Single store analysis\n",
        "async def demo_single_store(store_url: str = \"https://memy.co.in\"):\n",
        "    print(\"üöÄ Demo: Single Store Analysis\")\n",
        "    print(f\"Target: {store_url}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    try:\n",
        "        insights = await fetcher.analyze_store(store_url)\n",
        "        fetcher.display_insights(insights)\n",
        "\n",
        "        # Export to JSON\n",
        "        json_file = fetcher.export_to_json(insights)\n",
        "\n",
        "        return insights\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error analyzing store: {e}\")\n",
        "        return None\n",
        "\n",
        "# Demo 2: Multiple stores comparison\n",
        "async def demo_multiple_stores(store_urls: List[str] = None):\n",
        "    if store_urls is None:\n",
        "        store_urls = [\n",
        "            \"https://memy.co.in\",\n",
        "            \"https://hairoriginals.com\"\n",
        "        ]\n",
        "\n",
        "    print(\"üöÄ Demo: Multiple Stores Analysis\")\n",
        "    print(f\"Analyzing {len(store_urls)} stores...\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    all_insights = []\n",
        "\n",
        "    for i, url in enumerate(store_urls, 1):\n",
        "        print(f\"\\nüìç [{i}/{len(store_urls)}] Analyzing: {url}\")\n",
        "        try:\n",
        "            insights = await fetcher.analyze_store(url)\n",
        "            all_insights.append(insights)\n",
        "            print(f\"‚úÖ Successfully analyzed {insights.store_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error analyzing {url}: {e}\")\n",
        "\n",
        "    # Create summary DataFrame\n",
        "    if all_insights:\n",
        "        summary_df = fetcher.create_summary_dataframe(all_insights)\n",
        "        print(\"\\nüìä SUMMARY COMPARISON\")\n",
        "        print(\"=\" * 100)\n",
        "        print(summary_df.to_string(index=False))\n",
        "\n",
        "        # Export summary to CSV\n",
        "        summary_df.to_csv('stores_comparison.csv', index=False)\n",
        "        print(f\"\\nüìÅ Summary exported to stores_comparison.csv\")\n",
        "\n",
        "    return all_insights\n",
        "\n",
        "# Demo 3: Quick analysis\n",
        "async def quick_analyze(store_url: str):\n",
        "    \"\"\"Quick analysis with minimal output\"\"\"\n",
        "    print(f\"üîç Quick analysis of: {store_url}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    try:\n",
        "        insights = await fetcher.analyze_store(store_url)\n",
        "\n",
        "        print(f\"\\nüìä QUICK SUMMARY:\")\n",
        "        print(f\"üè™ Store: {insights.store_name}\")\n",
        "        print(f\"üì¶ Products: {len(insights.product_catalog)}\")\n",
        "        print(f\"üè† Hero Products: {len(insights.hero_products)}\")\n",
        "        print(f\"üì± Social Handles: {len(insights.social_handles)}\")\n",
        "        print(f\"‚ùì FAQs: {len(insights.faqs)}\")\n",
        "        print(f\"üìú Policies: {sum([bool(insights.privacy_policy), bool(insights.return_policy), bool(insights.refund_policy)])}/3\")\n",
        "        print(f\"üîó Important Links: {len(insights.important_links)}\")\n",
        "        print(f\"üìû Contact Info: {len(insights.contact_info.emails)} emails, {len(insights.contact_info.phone_numbers)} phones\")\n",
        "\n",
        "        return insights\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ Demo functions ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlINs4orZ-Vk",
        "outputId": "fde85cf7-a3f2-4495-d209-12097c82b318"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Demo functions ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: RUN THE DEMO\n",
        "\n",
        "# Method 1: Analyze a single store (memy.co.in)\n",
        "print(\"üéØ STARTING SINGLE STORE ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "insights = await demo_single_store(\"https://memy.co.in\")\n",
        "\n",
        "if insights:\n",
        "    print(\"\\nüéâ Analysis completed successfully!\")\n",
        "    print(f\"üìä Results Summary:\")\n",
        "    print(f\"   Products found: {len(insights.product_catalog)}\")\n",
        "    print(f\"   Hero products: {len(insights.hero_products)}\")\n",
        "    print(f\"   Social handles: {len(insights.social_handles)}\")\n",
        "    print(f\"   FAQs: {len(insights.faqs)}\")\n",
        "    print(f\"   Policies: {sum([bool(insights.privacy_policy), bool(insights.return_policy), bool(insights.refund_policy)])}\")\n",
        "else:\n",
        "    print(\"‚ùå Analysis failed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReqQWR8nZ-TV",
        "outputId": "770c1c24-ab71-4424-af3a-215d1405b6c5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ STARTING SINGLE STORE ANALYSIS\n",
            "============================================================\n",
            "üöÄ Demo: Single Store Analysis\n",
            "Target: https://memy.co.in\n",
            "--------------------------------------------------\n",
            "üîç Starting to scrape: https://memy.co.in\n",
            "üì¶ Scraping product catalog...\n",
            "‚úÖ Found 30 products\n",
            "üè† Scraping hero products...\n",
            "‚úÖ Found 7 hero products\n",
            "üìú Scraping policies...\n",
            "‚ùì Scraping FAQs...\n",
            "‚úÖ Found 0 FAQs\n",
            "üì± Scraping social handles...\n",
            "‚úÖ Found 2 social handles\n",
            "üìû Scraping contact info...\n",
            "‚ÑπÔ∏è Scraping about brand...\n",
            "üîó Scraping important links...\n",
            "‚úÖ Scraping completed successfully!\n",
            "================================================================================\n",
            "üè™ STORE ANALYSIS: Memy\n",
            "üåê URL: https://memy.co.in\n",
            "üìÖ Scraped: 2025-08-16 09:48:54.605912\n",
            "================================================================================\n",
            "\n",
            "üì¶ PRODUCT CATALOG (30 products)\n",
            "  1. Royal Purple Cotton Printed Kurta Set with Dupatta\n",
            "     Price: $1299.0 | Vendor: Me & My\n",
            "     Type: 3pc Set | Available: True\n",
            "     Tags: 3pc, 3PC_1 LOT, 3XL\n",
            "\n",
            "  2. Frost Shell Off-White Cotton Flex Co-ord Set\n",
            "     Price: $999.0 | Vendor: Me & My\n",
            "     Type: Co-ord Set | Available: True\n",
            "     Tags: best, CFCS_2 LOT, Co-ord set\n",
            "\n",
            "  3. Pearl Mist Off-White Cotton Flex Co-ord Set\n",
            "     Price: $999.0 | Vendor: Me & My\n",
            "     Type: Co-ord Set | Available: True\n",
            "     Tags: best, CFCS_2 LOT, Co-ord set\n",
            "\n",
            "  4. Porcelain Drift Off-White Cotton Flex Co-ord Set\n",
            "     Price: $999.0 | Vendor: Me & My\n",
            "     Type: Co-ord Set | Available: True\n",
            "     Tags: best, CFCS_2 LOT, Co-ord set\n",
            "\n",
            "  5. Oyster Calm Off-White Cotton Flex Co-ord Set\n",
            "     Price: $999.0 | Vendor: Me & My\n",
            "     Type: Co-ord Set | Available: True\n",
            "     Tags: best, CFCS_2 LOT, Co-ord set\n",
            "\n",
            "  ... and 25 more products\n",
            "\n",
            "üè† HERO PRODUCTS (7 products)\n",
            "  1. Blue Cotton Abstract Co-ord Set - $699.0\n",
            "  2. Off-White Cotton Floral Co-ord Set - $759.0\n",
            "  3. Garden Breeze White Botanical Cotton Co-ord Set - $799.0\n",
            "  4. Sage Serenity Green Cotton Co-ord Set - $799.0\n",
            "  5. Luxe Ease Blue Cotton Co-ord Set - $999.0\n",
            "  6. Whisperleaf Off-White Botanical Co-ord Set - $999.0\n",
            "  7. Blue Cotton Botanical Co-ord Set - $699.0\n",
            "\n",
            "üì± SOCIAL HANDLES (2 handles)\n",
            "  Facebook: https://www.facebook.com/memy.co.in\n",
            "    Username: @memy.co.in\n",
            "  Instagram: https://www.instagram.com/memy.co.in/\n",
            "    Username: @memy.co.in\n",
            "\n",
            "üìû CONTACT INFO\n",
            "  üìß Emails: orders@memy.co.in, care@memy.co.in\n",
            "  üì± Phones: , +91, 1\n",
            "\n",
            "‚ùì FAQs (0 found)\n",
            "\n",
            "üîó IMPORTANT LINKS (4 found)\n",
            "  Shipping Policy: https://memy.co.in/policies/shipping-policy\n",
            "  Contact Us: https://memy.co.in/pages/contact\n",
            "  Size Guide: https://memy.co.in/pages/size-guide\n",
            "  Track My Order: https://memy.shiprocket.co/tracking\n",
            "\n",
            "üìú POLICIES\n",
            "  ‚úÖ Privacy Policy: 2000 characters\n",
            "  ‚úÖ Refund Policy: 2000 characters\n",
            "\n",
            "‚ÑπÔ∏è ABOUT BRAND\n",
            "  About us At Me & My, we believe that fashion should be fun, accessible, and affordable. Founded by two passionate individuals, Vandan Mehta and Naman Mehta, we set out on a mission to redefine the way women shop for western apparel. Our collection is carefully curated to offer a diverse range of sty...\n",
            "================================================================================\n",
            "üìÅ Insights exported to Memy_insights.json\n",
            "\n",
            "üéâ Analysis completed successfully!\n",
            "üìä Results Summary:\n",
            "   Products found: 30\n",
            "   Hero products: 7\n",
            "   Social handles: 2\n",
            "   FAQs: 0\n",
            "   Policies: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8: ANALYZE ANOTHER STORE\n",
        "# You can analyze any other Shopify store by changing the URL\n",
        "custom_store_url = \"https://hairoriginals.com\"  # Change this to any Shopify store\n",
        "\n",
        "print(f\"\\nüéØ ANALYZING CUSTOM STORE: {custom_store_url}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "custom_insights = await demo_single_store(custom_store_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDlwDcx7bOkk",
        "outputId": "b29f5de6-8eed-4555-ef00-b24da25e9285"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ ANALYZING CUSTOM STORE: https://hairoriginals.com\n",
            "============================================================\n",
            "üöÄ Demo: Single Store Analysis\n",
            "Target: https://hairoriginals.com\n",
            "--------------------------------------------------\n",
            "üîç Starting to scrape: https://hairoriginals.com\n",
            "üì¶ Scraping product catalog...\n",
            "‚úÖ Found 30 products\n",
            "üè† Scraping hero products...\n",
            "‚úÖ Found 10 hero products\n",
            "üìú Scraping policies...\n",
            "‚ùì Scraping FAQs...\n",
            "‚úÖ Found 20 FAQs\n",
            "üì± Scraping social handles...\n",
            "‚úÖ Found 3 social handles\n",
            "üìû Scraping contact info...\n",
            "‚ÑπÔ∏è Scraping about brand...\n",
            "üîó Scraping important links...\n",
            "‚úÖ Scraping completed successfully!\n",
            "================================================================================\n",
            "üè™ STORE ANALYSIS: Hairoriginals\n",
            "üåê URL: https://hairoriginals.com\n",
            "üìÖ Scraped: 2025-08-16 09:48:54.605912\n",
            "================================================================================\n",
            "\n",
            "üì¶ PRODUCT CATALOG (30 products)\n",
            "  1. Wavy Topper Hair Extensions For Women | Wavy & 100% Real Hair For Crown Area\n",
            "     Price: $11399.0 | Vendor: HairOriginals\n",
            "     Type:  | Available: True\n",
            "     Tags: Hair Extension, Hair Originals\n",
            "\n",
            "  2. Bow Clips\n",
            "     Price: $200.0 | Vendor: HairOriginals\n",
            "     Type:  | Available: True\n",
            "\n",
            "  3. Clip-In Hair Streaks | Pack of 4 | 100% Human Hair Extensions\n",
            "     Price: $1350.0 | Vendor: HairOriginals\n",
            "     Type:  | Available: True\n",
            "     Tags: BOB_Customer_favourites\n",
            "\n",
            "  4. Balayage Hair Streaks | Colorful Hair Without Any Damage | Pack of 2\n",
            "     Price: $700.0 | Vendor: HairOriginals\n",
            "     Type:  | Available: True\n",
            "     Tags: Color streaks, Hair Extension, Hair Originals\n",
            "\n",
            "  5. Clip-In Hair Streaks | Pack of 2 | 100% Human Hair Extensions\n",
            "     Price: $700.0 | Vendor: HairOriginals\n",
            "     Type:  | Available: True\n",
            "     Tags: BOB_Customer_favourites\n",
            "\n",
            "  ... and 25 more products\n",
            "\n",
            "üè† HERO PRODUCTS (10 products)\n",
            "  1. Highlighted Scalp Topper Hair Extensions| Hide Hair Thinning For Scalp Area - $9499.0\n",
            "  2. Hair Topper With Bangs Hair Extensions | Silk Base & 100% Original Hair Extensions - $2699.0\n",
            "  3. Silk-Based Wig hair Extensions | Premium Quality 100% Human Hair Wigs For Women - $19499.0\n",
            "  4. Invisible Hair Patch - $1099.0\n",
            "  5. Thick Hair Patch - $2299.0\n",
            "  6. Side Patches | Pair of Hair Extensions - $2399.0\n",
            "  7. Hair Fiber | Transform Thin, And Fine Hair With Hair Building Fiber | Hair Volumizing Fibers For Women & Men - $699.0\n",
            "  8. Hairline Powder | Instantly Conceals Hair Loss | Root Touch Up Hair Powder for Women & Men - $499.0\n",
            "  9. Messy Bun Scrunchie - $999.0\n",
            "  10. Bangs Without Temple | Trendy Korean Bangs - $999.0\n",
            "\n",
            "üì± SOCIAL HANDLES (3 handles)\n",
            "  Facebook: https://www.facebook.com/hairoriginals\n",
            "    Username: @hairoriginals\n",
            "  Instagram: https://www.instagram.com/hairoriginals/\n",
            "    Username: @hairoriginals\n",
            "  Youtube: https://www.youtube.com/@hairoriginals.\n",
            "    Username: @@hairoriginals.\n",
            "\n",
            "üìû CONTACT INFO\n",
            "  üìß Emails: feedback@hairoriginals.com\n",
            "  üì± Phones: , 1, 1 \n",
            "\n",
            "‚ùì FAQs (15 found)\n",
            "  Q1: SHOP FOR WOMEN\n",
            "  A1: SHOP FOR WOMENDIY HAIR EXTENSIONSDIY HAIR EXTENSIONSBunsBangsStreaksVolumizers & Clip SetsHalo Hair ExtensionsSide PatchesPonyTailHAIR LOSS SOLUTIONHA...\n",
            "\n",
            "  Q2: DIY HAIR EXTENSIONS\n",
            "  A2: DIY HAIR EXTENSIONSBunsBangsStreaksVolumizers & Clip SetsHalo Hair ExtensionsSide PatchesPonyTail...\n",
            "\n",
            "  Q3: HAIR LOSS SOLUTION\n",
            "  A3: HAIR LOSS SOLUTIONScalp TopperHighlighted Scalp TopperTopper With BangsWigsInvisible Side PatchThick Hair PatchSide PatchHair FiberHairline Powder...\n",
            "\n",
            "\n",
            "üîó IMPORTANT LINKS (5 found)\n",
            "  Contact: https://hairoriginals.com/pages/contact\n",
            "  Blogs: https://hairoriginals.com/pages/blogs\n",
            "  Track Your Order: https://hairoriginals.com/pages/track-order\n",
            "  Contact Us: https://hairoriginals.com/pages/contact\n",
            "  Shipping Policy: https://hairoriginals.com/pages/shipping-policy\n",
            "\n",
            "üìú POLICIES\n",
            "  ‚úÖ Privacy Policy: 2000 characters\n",
            "  ‚úÖ Refund Policy: 2000 characters\n",
            "\n",
            "‚ÑπÔ∏è ABOUT BRAND\n",
            "  Our Story About Us Established as a Pvt. Limited Company firm in the year 2018,‚ÄúKrisoriginals Private Limited‚ÄùHeadquartered in Gurgaon (Haryana, India)is a leading Manufacturer and Exporter of a wide range of Hair Extensions. We have constructed a wide and well-functional infrastructural unit pan In...\n",
            "================================================================================\n",
            "üìÅ Insights exported to Hairoriginals_insights.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 9: COMPARE MULTIPLE STORES\n",
        "# Compare multiple stores\n",
        "store_list = [\n",
        "    \"https://memy.co.in\",\n",
        "    \"https://hairoriginals.com\"\n",
        "]\n",
        "\n",
        "print(\"\\nüéØ COMPARING MULTIPLE STORES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "comparison_results = await demo_multiple_stores(store_list)\n",
        "\n",
        "if comparison_results:\n",
        "    print(f\"\\nüéâ Comparison completed! Analyzed {len(comparison_results)} stores successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JTyO9Q-Z-Qx",
        "outputId": "d366a359-b666-4446-c339-22a56d17e058"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ COMPARING MULTIPLE STORES\n",
            "============================================================\n",
            "üöÄ Demo: Multiple Stores Analysis\n",
            "Analyzing 2 stores...\n",
            "--------------------------------------------------\n",
            "\n",
            "üìç [1/2] Analyzing: https://memy.co.in\n",
            "üîç Starting to scrape: https://memy.co.in\n",
            "üì¶ Scraping product catalog...\n",
            "‚úÖ Found 30 products\n",
            "üè† Scraping hero products...\n",
            "‚úÖ Found 7 hero products\n",
            "üìú Scraping policies...\n",
            "‚ùì Scraping FAQs...\n",
            "‚úÖ Found 0 FAQs\n",
            "üì± Scraping social handles...\n",
            "‚úÖ Found 2 social handles\n",
            "üìû Scraping contact info...\n",
            "‚ÑπÔ∏è Scraping about brand...\n",
            "üîó Scraping important links...\n",
            "‚úÖ Scraping completed successfully!\n",
            "‚úÖ Successfully analyzed Memy\n",
            "\n",
            "üìç [2/2] Analyzing: https://hairoriginals.com\n",
            "üîç Starting to scrape: https://hairoriginals.com\n",
            "üì¶ Scraping product catalog...\n",
            "‚úÖ Found 30 products\n",
            "üè† Scraping hero products...\n",
            "‚úÖ Found 10 hero products\n",
            "üìú Scraping policies...\n",
            "‚ùì Scraping FAQs...\n",
            "‚úÖ Found 20 FAQs\n",
            "üì± Scraping social handles...\n",
            "‚úÖ Found 3 social handles\n",
            "üìû Scraping contact info...\n",
            "‚ÑπÔ∏è Scraping about brand...\n",
            "üîó Scraping important links...\n",
            "‚úÖ Scraping completed successfully!\n",
            "‚úÖ Successfully analyzed Hairoriginals\n",
            "\n",
            "üìä SUMMARY COMPARISON\n",
            "====================================================================================================\n",
            "   Store Name                       URL  Total Products  Hero Products  Social Handles  FAQs  Has Privacy Policy  Has Return Policy  Has About Section  Contact Emails  Contact Phones  Important Links          Scraped At\n",
            "         Memy        https://memy.co.in              30              7               2     0                True              False               True               2               3                4 2025-08-16 09:48:54\n",
            "Hairoriginals https://hairoriginals.com              30             10               3    15                True              False               True               1               3                5 2025-08-16 09:48:54\n",
            "\n",
            "üìÅ Summary exported to stores_comparison.csv\n",
            "\n",
            "üéâ Comparison completed! Analyzed 2 stores successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 10: EXPORT AND DOWNLOAD RESULTS\n",
        "\n",
        "# Function to display files created\n",
        "import os\n",
        "\n",
        "def show_generated_files():\n",
        "    print(\"\\nüìÅ GENERATED FILES:\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    json_files = [f for f in os.listdir('.') if f.endswith('_insights.json')]\n",
        "    csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
        "\n",
        "    print(\"üìÑ JSON Files (Store Insights):\")\n",
        "    for file in json_files:\n",
        "        size = os.path.getsize(file) / 1024  # Size in KB\n",
        "        print(f\"   {file} ({size:.1f} KB)\")\n",
        "\n",
        "    print(\"\\nüìä CSV Files (Comparisons):\")\n",
        "    for file in csv_files:\n",
        "        size = os.path.getsize(file) / 1024  # Size in KB\n",
        "        print(f\"   {file} ({size:.1f} KB)\")\n",
        "\n",
        "    print(f\"\\nüíæ Total files generated: {len(json_files) + len(csv_files)}\")\n",
        "\n",
        "# Show all generated files\n",
        "show_generated_files()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CVAvna7Z-OY",
        "outputId": "926151e6-ec65-4ccd-8d2b-e2a7c6209fce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìÅ GENERATED FILES:\n",
            "========================================\n",
            "üìÑ JSON Files (Store Insights):\n",
            "   Hairoriginals_insights.json (119.1 KB)\n",
            "   Memy_insights.json (95.6 KB)\n",
            "\n",
            "üìä CSV Files (Comparisons):\n",
            "   stores_comparison.csv (0.3 KB)\n",
            "\n",
            "üíæ Total files generated: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf = pd.read_csv('stores_comparison.csv')"
      ],
      "metadata": {
        "id": "RBeg37lfZ-Ly"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pdf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHqwJvs9Z-JR",
        "outputId": "a7363bb9-3ce2-42dc-f581-80677b27f439"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Store Name                        URL  Total Products  Hero Products  \\\n",
            "0           Memy         https://memy.co.in              30              7   \n",
            "1  Hairoriginals  https://hairoriginals.com              30             10   \n",
            "\n",
            "   Social Handles  FAQs  Has Privacy Policy  Has Return Policy  \\\n",
            "0               2     0                True              False   \n",
            "1               3    15                True              False   \n",
            "\n",
            "   Has About Section  Contact Emails  Contact Phones  Important Links  \\\n",
            "0               True               2               3                4   \n",
            "1               True               1               3                5   \n",
            "\n",
            "            Scraped At  \n",
            "0  2025-08-16 09:48:54  \n",
            "1  2025-08-16 09:48:54  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OvC8RTkwZ-G7"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}